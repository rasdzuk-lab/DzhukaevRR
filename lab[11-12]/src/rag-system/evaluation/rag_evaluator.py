# evaluation/rag_evaluator.py
import pandas as pd
import json
from datetime import datetime
from typing import List, Dict, Any
import logging
import asyncio
import os

from .test_dataset import TestCase, EvaluationDataset
from .retrieval_evaluator import RetrievalEvaluator
from .generation_evaluator import GenerationEvaluator
from pipeline.rag_pipeline import RAGPipeline

logger = logging.getLogger(__name__)

class RAGEvaluator:
    def __init__(self, rag_pipeline: RAGPipeline):
        self.pipeline = rag_pipeline
        self.retrieval_evaluator = RetrievalEvaluator(rag_pipeline.retriever)
        self.generation_evaluator = GenerationEvaluator()
        self.dataset = EvaluationDataset()
    
    async def run_comprehensive_evaluation(self, test_cases: List[TestCase] = None) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ RAG-—Å–∏—Å—Ç–µ–º—ã"""
        if test_cases is None:
            test_cases = self.dataset.test_cases 
        
        logger.info(f"Starting comprehensive evaluation with {len(test_cases)} test cases")
    
        # –≠—Ç–∞–ø 1: –û—Ü–µ–Ω–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        retrieval_results = self.retrieval_evaluator.evaluate_dataset(test_cases)
    
        # –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
        generated_answers = []
        loop = asyncio.get_event_loop()
        for test_case in test_cases:
            try:
                result = await self.pipeline.process_question(test_case.question)
                generated_answers.append(result["answer"])
            except Exception as e:
                logger.error(f"Error processing question: {test_case.question}, error: {e}")
                generated_answers.append("") # –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                
        # –≠—Ç–∞–ø 3: –û—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_results = self.generation_evaluator.evaluate_answers(
            test_cases, generated_answers
        )
    
        # –≠—Ç–∞–ø 4: –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_report = self._compile_final_report(
            retrieval_results,
            generation_results,
            test_cases
        )
    
        return final_report

    async def evaluate_by_category(self) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤"""
        categories = set(case.category for case in self.dataset.test_cases)
        category_results = {}

        for category in categories:
            category_cases = self.dataset.get_cases_by_category(category)
            if category_cases:
                results = await self.run_comprehensive_evaluation(category_cases)
                category_results[category] = results["aggregated_metrics"]
        
        return category_results
            
    async def evaluate_by_difficulty(self) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–æ–≤"""
        difficulties = ["easy", "medium", "hard"]
        difficulty_results = {}
        
        for difficulty in difficulties:
            difficulty_cases = self.dataset.get_cases_by_difficulty(difficulty)
            if difficulty_cases:
                results = await self.run_comprehensive_evaluation(difficulty_cases)
                difficulty_results[difficulty] = results["aggregated_metrics"]

        return difficulty_results

    def _compile_final_report(self, retrieval_results: Dict, generation_results: Dict, test_cases: List[TestCase]) -> Dict[str, Any]:
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        aggregated_metrics = {
            "retrieval": retrieval_results["aggregated_metrics"],
            "generation": generation_results["aggregated_metrics"],
            "overall_score": self._calculate_overall_score(
                retrieval_results["aggregated_metrics"],
                generation_results["aggregated_metrics"]
            )
        }
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        detailed_results = []
        for retrieval, generation in zip(
            retrieval_results["detailed_results"],
            generation_results["detailed_results"]
        ):
            combined = {**retrieval, **generation}
            detailed_results.append(combined)
        
        report = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "test_cases_count": len(test_cases),
            "aggregated_metrics": aggregated_metrics,
            "detailed_results": detailed_results,
            "dataset_statistics": self.dataset.get_statistics()
        }
        return report
    
    def _calculate_overall_score(self, retrieval_metrics: Dict, generation_metrics: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ —Å–∫–æ—Ä–∞ —Å–∏—Å—Ç–µ–º—ã"""
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        weights = {
            "retrieval_precision": 0.3,
            "retrieval_recall": 0.2,
            "generation_semantic_similarity": 0.3,
            "generation_rougeL": 0.2
        }

        score = 0
        score += retrieval_metrics.get("mean_precision", 0) * weights["retrieval_precision"]
        score += retrieval_metrics.get("mean_recall", 0) * weights["retrieval_recall"]
        score += generation_metrics.get("mean_semantic_similarity", 0) * weights["generation_semantic_similarity"]
        score += generation_metrics.get("mean_rougeL", 0) * weights["generation_rougeL"]
        
        return score

    def save_report(self, report: Dict[str, Any], filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_evaluation_report_{timestamp}.json"

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        path = filename.rsplit('/', 1)[0]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏ –µ–µ —Å–æ–∑–¥–∞–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not os.path.isdir(path):
            os.mkdir(path)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Evaluation report saved to {filename}")
        
    def generate_summary(self, report: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å–≤–æ–¥–∫–∏"""
        metrics = report["aggregated_metrics"]

        summary = f"""
üìä –û–¢–ß–ï–¢ –û–¶–ï–ù–ö–ò RAG-–°–ò–°–¢–ï–ú–´
===========================

–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
- –í—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏: {report['evaluation_timestamp']}
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤: {report['test_cases_count']}
- –û–±—â–∏–π score —Å–∏—Å—Ç–µ–º—ã: {metrics['overall_score']:.3f}

–ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ (Retrieval):
- Precision: {metrics['retrieval']['mean_precision']:.3f} ¬± {metrics['retrieval']['std_precision']:.3f}
- Recall: {metrics['retrieval']['mean_recall']:.3f} ¬± {metrics['retrieval']['std_recall']:.3f}
- F1-Score: {metrics['retrieval']['mean_f1_score']:.3f} ¬± {metrics['retrieval']['std_f1_score']:.3f}
- MRR: {metrics['retrieval']['mean_mrr']:.3f} ¬± {metrics['retrieval']['std_mrr']:.3f}

–ö–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (Generation):
- ROUGE-L: {metrics['generation']['mean_rougeL']:.3f} ¬± {metrics['generation']['std_rougeL']:.3f}
- BLEU: {metrics['generation']['mean_bleu']:.3f} ¬± {metrics['generation'] ['std_bleu']:.3f}
- Semantic Similarity: {metrics['generation']
['mean_semantic_similarity']:.3f} ¬± {metrics['generation']['std_semantic_similarity']:.3f}
- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {metrics['generation']['mean_answer_length']:.1f}
—Å–ª–æ–≤

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:
{self._generate_recommendations(metrics)}
        """
        return summary
    
    def _generate_recommendations(self, metrics: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        if metrics['retrieval']['mean_precision'] < 0.7:
            recommendations.append("‚Ä¢ –£–ª—É—á—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞: –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥-–º–æ–¥–µ–ª—å –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    
        if metrics['retrieval']['mean_recall'] < 0.6:
            recommendations.append("‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –ø–æ–∏—Å–∫–∞: —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    
        if metrics['generation']['mean_semantic_similarity'] < 0.7:
            recommendations.append("‚Ä¢ –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –º–æ—â–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å")
    
        if metrics['generation']['mean_rougeL'] < 0.4:
            recommendations.append("‚Ä¢ –†–∞–±–æ—Ç–∞—Ç—å –Ω–∞–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º: –¥–æ–±–∞–≤–∏—Ç—å few-shot –ø—Ä–∏–º–µ—Ä—ã –≤ –ø—Ä–æ–º–ø—Ç—ã")
    
        if not recommendations:
            recommendations.append("‚Ä¢ –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞.")
    
        return "\n".join(recommendations)